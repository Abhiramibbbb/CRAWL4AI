{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CXUG53IfhxPA",
        "outputId": "89c8c767-9015-48de-92e0-39c047fbbac5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crawl4ai@ git+https://github.com/unclecode/crawl4ai.git\n",
            "  Cloning https://github.com/unclecode/crawl4ai.git to /tmp/pip-install-v9etbdej/crawl4ai_19af2dc538ea4069af9b721487f39104\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unclecode/crawl4ai.git /tmp/pip-install-v9etbdej/crawl4ai_19af2dc538ea4069af9b721487f39104\n",
            "  Resolved https://github.com/unclecode/crawl4ai.git to commit 0afc3e9e5e38b09d0995042ecaa9c77de66842e1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiosqlite~=0.20 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading aiosqlite-0.20.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (5.3.0)\n",
            "Collecting litellm>=1.53.1 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading litellm-1.59.6-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.26.4)\n",
            "Collecting pillow~=10.4 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting playwright>=1.49.0 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting python-dotenv~=1.0 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: requests~=2.26 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (4.12.3)\n",
            "Collecting tf-playwright-stealth>=1.1.0 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting xxhash~=3.4 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rank-bm25~=0.2 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting aiofiles>=24.1.0 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting colorama~=0.4 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.2.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.10.5)\n",
            "Collecting pyOpenSSL>=24.3.0 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading pyOpenSSL-25.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting psutil>=6.1.1 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.9.1)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (13.9.4)\n",
            "Collecting cssselect>=1.2.0 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting httpx==0.27.2 (from crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx==0.27.2->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.14.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite~=0.20->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4~=4.12->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.11.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (8.1.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.1.5)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.55.3 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.59.6)\n",
            "Collecting tiktoken>=0.7.0 (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (4.67.1)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.1.1)\n",
            "Collecting pyee==12.0.0 (from playwright>=1.49.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading pyee-12.0.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.27.2)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (43.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.18.0)\n",
            "Collecting fake-http-header<0.4.0,>=0.3.5 (from tf-playwright-stealth>=1.1.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading fake_http_header-0.3.5-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting pytest-mockito<0.0.5,>=0.0.4 (from tf-playwright-stealth>=1.1.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading pytest-mockito-0.0.4.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.17.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.8.2)\n",
            "Requirement already satisfied: pytest>=3 in /usr/local/lib/python3.11/dist-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (8.3.4)\n",
            "Collecting mockito>=1.0.6 (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git)\n",
            "  Downloading mockito-1.5.4-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.18.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (0.27.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (6.0.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai@ git+https://github.com/unclecode/crawl4ai.git) (1.5.0)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m335.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading aiosqlite-0.20.0-py3-none-any.whl (15 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading litellm-1.59.6-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading playwright-1.49.1-py3-none-manylinux1_x86_64.whl (44.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyee-12.0.0-py3-none-any.whl (14 kB)\n",
            "Downloading psutil-6.1.1-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading tf_playwright_stealth-1.1.0-py3-none-any.whl (33 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_http_header-0.3.5-py3-none-any.whl (14 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mockito-1.5.4-py3-none-any.whl (30 kB)\n",
            "Building wheels for collected packages: crawl4ai, pytest-mockito\n",
            "  Building wheel for crawl4ai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crawl4ai: filename=Crawl4AI-0.4.3b2-py3-none-any.whl size=181853 sha256=71989cc1159e6ba5ab7047425e9db24e16df342e0694346cb0c69d31c016d66e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8y83tar4/wheels/0e/21/a2/6f31e4b8269779784418d6b0aa11cd80dc7df2fbb282e52e42\n",
            "  Building wheel for pytest-mockito (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytest-mockito: filename=pytest_mockito-0.0.4-py3-none-any.whl size=3700 sha256=782f2c140bfee79014a13cffe23fe97efc2bd7603c5abca328290a64add10e64\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/ca/3b/b78048d21f5c607308e914700947f32a202115457dad10d5bb\n",
            "Successfully built crawl4ai pytest-mockito\n",
            "Installing collected packages: xxhash, rank-bm25, python-dotenv, pyee, psutil, pillow, mockito, fake-http-header, cssselect, colorama, aiosqlite, aiofiles, tiktoken, pytest-mockito, playwright, httpx, tf-playwright-stealth, pyOpenSSL, litellm, crawl4ai\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.9.5\n",
            "    Uninstalling psutil-5.9.5:\n",
            "      Successfully uninstalled psutil-5.9.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: pyOpenSSL\n",
            "    Found existing installation: pyOpenSSL 24.2.1\n",
            "    Uninstalling pyOpenSSL-24.2.1:\n",
            "      Successfully uninstalled pyOpenSSL-24.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 aiosqlite-0.20.0 colorama-0.4.6 crawl4ai-0.4.3b2 cssselect-1.2.0 fake-http-header-0.3.5 httpx-0.27.2 litellm-1.59.6 mockito-1.5.4 pillow-10.4.0 playwright-1.49.1 psutil-6.1.1 pyOpenSSL-25.0.0 pyee-12.0.0 pytest-mockito-0.0.4 python-dotenv-1.0.1 rank-bm25-0.2.2 tf-playwright-stealth-1.1.0 tiktoken-0.8.0 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "psutil"
                ]
              },
              "id": "92252f6144e9475ba6a24c781d43a596"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install \"crawl4ai @ git+https://github.com/unclecode/crawl4ai.git\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install crawl4ai and dependencies (if not already installed)\n",
        "!pip install crawl4ai\n",
        "!playwright install  # Required for browser automation\n",
        "!pip install nest_asyncio  # To support asyncio in environments like Colab or Jupyter\n",
        "\n",
        "# Import nest_asyncio and apply it to allow asyncio in Colab/Jupyter environments\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "print('Setup complete!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbMu-gDZiM9W",
        "outputId": "9bf3ff00-3a11-4220-db70-7d90d89a25eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: crawl4ai in /usr/local/lib/python3.11/dist-packages (0.4.3b2)\n",
            "Requirement already satisfied: aiosqlite~=0.20 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.20.0)\n",
            "Requirement already satisfied: lxml~=5.3 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (5.3.0)\n",
            "Requirement already satisfied: litellm>=1.53.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.59.6)\n",
            "Requirement already satisfied: numpy<3,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.26.4)\n",
            "Requirement already satisfied: pillow~=10.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (10.4.0)\n",
            "Requirement already satisfied: playwright>=1.49.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.49.1)\n",
            "Requirement already satisfied: python-dotenv~=1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.0.1)\n",
            "Requirement already satisfied: requests~=2.26 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4~=4.12 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (4.12.3)\n",
            "Requirement already satisfied: tf-playwright-stealth>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.1.0)\n",
            "Requirement already satisfied: xxhash~=3.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.5.0)\n",
            "Requirement already satisfied: rank-bm25~=0.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.2.2)\n",
            "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (24.1.0)\n",
            "Requirement already satisfied: colorama~=0.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.4.6)\n",
            "Requirement already satisfied: snowballstemmer~=2.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.2.0)\n",
            "Requirement already satisfied: pydantic>=2.10 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (2.10.5)\n",
            "Requirement already satisfied: pyOpenSSL>=24.3.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (25.0.0)\n",
            "Requirement already satisfied: psutil>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (6.1.1)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (3.9.1)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (13.9.4)\n",
            "Requirement already satisfied: cssselect>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (1.2.0)\n",
            "Requirement already satisfied: httpx==0.27.2 in /usr/local/lib/python3.11/dist-packages (from crawl4ai) (0.27.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.27.2->crawl4ai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx==0.27.2->crawl4ai) (0.14.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.11/dist-packages (from aiosqlite~=0.20->crawl4ai) (4.12.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4~=4.12->crawl4ai) (2.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (3.11.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (8.1.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (8.5.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (3.1.5)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (4.23.0)\n",
            "Requirement already satisfied: openai>=1.55.3 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (1.59.6)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.8.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm>=1.53.1->crawl4ai) (0.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->crawl4ai) (4.67.1)\n",
            "Requirement already satisfied: greenlet==3.1.1 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (3.1.1)\n",
            "Requirement already satisfied: pyee==12.0.0 in /usr/local/lib/python3.11/dist-packages (from playwright>=1.49.0->crawl4ai) (12.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.10->crawl4ai) (2.27.2)\n",
            "Requirement already satisfied: cryptography<45,>=41.0.5 in /usr/local/lib/python3.11/dist-packages (from pyOpenSSL>=24.3.0->crawl4ai) (43.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.26->crawl4ai) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.9.4->crawl4ai) (2.18.0)\n",
            "Requirement already satisfied: fake-http-header<0.4.0,>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.3.5)\n",
            "Requirement already satisfied: pytest-mockito<0.0.5,>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from tf-playwright-stealth>=1.1.0->crawl4ai) (0.0.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (1.17.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm>=1.53.1->crawl4ai) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.53.1->crawl4ai) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.53.1->crawl4ai) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->crawl4ai) (0.1.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.55.3->litellm>=1.53.1->crawl4ai) (0.8.2)\n",
            "Requirement already satisfied: pytest>=3 in /usr/local/lib/python3.11/dist-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (8.3.4)\n",
            "Requirement already satisfied: mockito>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm>=1.53.1->crawl4ai) (1.18.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm>=1.53.1->crawl4ai) (0.27.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45,>=41.0.5->pyOpenSSL>=24.3.0->crawl4ai) (2.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm>=1.53.1->crawl4ai) (6.0.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=3->pytest-mockito<0.0.5,>=0.0.4->tf-playwright-stealth>=1.1.0->crawl4ai) (1.5.0)\n",
            "Downloading Chromium 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G161.3 MiB [] 0% 10.7s\u001b[0K\u001b[1G161.3 MiB [] 0% 35.3s\u001b[0K\u001b[1G161.3 MiB [] 0% 65.7s\u001b[0K\u001b[1G161.3 MiB [] 0% 14.6s\u001b[0K\u001b[1G161.3 MiB [] 0% 8.3s\u001b[0K\u001b[1G161.3 MiB [] 1% 6.0s\u001b[0K\u001b[1G161.3 MiB [] 2% 4.9s\u001b[0K\u001b[1G161.3 MiB [] 2% 4.1s\u001b[0K\u001b[1G161.3 MiB [] 3% 3.8s\u001b[0K\u001b[1G161.3 MiB [] 4% 3.4s\u001b[0K\u001b[1G161.3 MiB [] 5% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 5% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 6% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 6% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 7% 3.1s\u001b[0K\u001b[1G161.3 MiB [] 8% 3.0s\u001b[0K\u001b[1G161.3 MiB [] 8% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 9% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 9% 2.9s\u001b[0K\u001b[1G161.3 MiB [] 10% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 11% 2.8s\u001b[0K\u001b[1G161.3 MiB [] 11% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 12% 2.7s\u001b[0K\u001b[1G161.3 MiB [] 13% 2.6s\u001b[0K\u001b[1G161.3 MiB [] 14% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 15% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 16% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 17% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 18% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 19% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 20% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 21% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 22% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 23% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 23% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 24% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 25% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 26% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 27% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 28% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 29% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 30% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 31% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 32% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 33% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 35% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 36% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 37% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 38% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 39% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 40% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 42% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 43% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 44% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 45% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 46% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 47% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 48% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 48% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 49% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 50% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 51% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 52% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 53% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 53% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 54% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 54% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 55% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 56% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 56% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 56% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 57% 2.4s\u001b[0K\u001b[1G161.3 MiB [] 57% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 58% 2.5s\u001b[0K\u001b[1G161.3 MiB [] 60% 2.3s\u001b[0K\u001b[1G161.3 MiB [] 61% 2.2s\u001b[0K\u001b[1G161.3 MiB [] 62% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 63% 2.1s\u001b[0K\u001b[1G161.3 MiB [] 64% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 65% 2.0s\u001b[0K\u001b[1G161.3 MiB [] 65% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 66% 1.9s\u001b[0K\u001b[1G161.3 MiB [] 66% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 67% 1.8s\u001b[0K\u001b[1G161.3 MiB [] 68% 1.7s\u001b[0K\u001b[1G161.3 MiB [] 69% 1.6s\u001b[0K\u001b[1G161.3 MiB [] 70% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 72% 1.5s\u001b[0K\u001b[1G161.3 MiB [] 73% 1.4s\u001b[0K\u001b[1G161.3 MiB [] 74% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 75% 1.3s\u001b[0K\u001b[1G161.3 MiB [] 76% 1.2s\u001b[0K\u001b[1G161.3 MiB [] 77% 1.1s\u001b[0K\u001b[1G161.3 MiB [] 79% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 80% 1.0s\u001b[0K\u001b[1G161.3 MiB [] 81% 0.9s\u001b[0K\u001b[1G161.3 MiB [] 82% 0.8s\u001b[0K\u001b[1G161.3 MiB [] 84% 0.7s\u001b[0K\u001b[1G161.3 MiB [] 85% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 86% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 87% 0.6s\u001b[0K\u001b[1G161.3 MiB [] 88% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 89% 0.5s\u001b[0K\u001b[1G161.3 MiB [] 90% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 91% 0.4s\u001b[0K\u001b[1G161.3 MiB [] 92% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 94% 0.3s\u001b[0K\u001b[1G161.3 MiB [] 95% 0.2s\u001b[0K\u001b[1G161.3 MiB [] 96% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 97% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 98% 0.1s\u001b[0K\u001b[1G161.3 MiB [] 99% 0.0s\u001b[0K\u001b[1G161.3 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium-1148\n",
            "Downloading Chromium Headless Shell 131.0.6778.33 (playwright build v1148)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1148/chromium-headless-shell-linux.zip\u001b[22m\n",
            "\u001b[1G100.9 MiB [] 0% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 0% 41.3s\u001b[0K\u001b[1G100.9 MiB [] 0% 25.2s\u001b[0K\u001b[1G100.9 MiB [] 0% 15.4s\u001b[0K\u001b[1G100.9 MiB [] 0% 7.2s\u001b[0K\u001b[1G100.9 MiB [] 2% 4.2s\u001b[0K\u001b[1G100.9 MiB [] 3% 3.0s\u001b[0K\u001b[1G100.9 MiB [] 4% 2.6s\u001b[0K\u001b[1G100.9 MiB [] 5% 2.2s\u001b[0K\u001b[1G100.9 MiB [] 7% 1.8s\u001b[0K\u001b[1G100.9 MiB [] 9% 1.7s\u001b[0K\u001b[1G100.9 MiB [] 10% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 11% 1.6s\u001b[0K\u001b[1G100.9 MiB [] 13% 1.5s\u001b[0K\u001b[1G100.9 MiB [] 14% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 15% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 16% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 17% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 18% 1.4s\u001b[0K\u001b[1G100.9 MiB [] 20% 1.3s\u001b[0K\u001b[1G100.9 MiB [] 21% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 22% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 23% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 24% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 26% 1.2s\u001b[0K\u001b[1G100.9 MiB [] 27% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 28% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 30% 1.1s\u001b[0K\u001b[1G100.9 MiB [] 32% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 33% 1.0s\u001b[0K\u001b[1G100.9 MiB [] 35% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 36% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 37% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 38% 0.9s\u001b[0K\u001b[1G100.9 MiB [] 40% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 41% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 42% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 44% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 46% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 46% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 47% 0.8s\u001b[0K\u001b[1G100.9 MiB [] 49% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 51% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 53% 0.7s\u001b[0K\u001b[1G100.9 MiB [] 54% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 57% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 58% 0.6s\u001b[0K\u001b[1G100.9 MiB [] 60% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 62% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 63% 0.5s\u001b[0K\u001b[1G100.9 MiB [] 65% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 67% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 69% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 70% 0.4s\u001b[0K\u001b[1G100.9 MiB [] 72% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 74% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 75% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 77% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 79% 0.3s\u001b[0K\u001b[1G100.9 MiB [] 80% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 82% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 84% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 86% 0.2s\u001b[0K\u001b[1G100.9 MiB [] 88% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 90% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 91% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 93% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 94% 0.1s\u001b[0K\u001b[1G100.9 MiB [] 96% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 98% 0.0s\u001b[0K\u001b[1G100.9 MiB [] 100% 0.0s\u001b[0K\n",
            "Chromium Headless Shell 131.0.6778.33 (playwright build v1148) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1148\n",
            "Downloading Firefox 132.0 (playwright build v1466)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1466/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G87.6 MiB [] 0% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 0% 22.5s\u001b[0K\u001b[1G87.6 MiB [] 0% 14.4s\u001b[0K\u001b[1G87.6 MiB [] 0% 8.9s\u001b[0K\u001b[1G87.6 MiB [] 1% 4.6s\u001b[0K\u001b[1G87.6 MiB [] 2% 3.3s\u001b[0K\u001b[1G87.6 MiB [] 3% 2.6s\u001b[0K\u001b[1G87.6 MiB [] 4% 2.3s\u001b[0K\u001b[1G87.6 MiB [] 6% 2.0s\u001b[0K\u001b[1G87.6 MiB [] 7% 1.8s\u001b[0K\u001b[1G87.6 MiB [] 9% 1.6s\u001b[0K\u001b[1G87.6 MiB [] 11% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 12% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 12% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 13% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 14% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 16% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 17% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 18% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 19% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 20% 1.5s\u001b[0K\u001b[1G87.6 MiB [] 22% 1.4s\u001b[0K\u001b[1G87.6 MiB [] 23% 1.3s\u001b[0K\u001b[1G87.6 MiB [] 25% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 26% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 28% 1.2s\u001b[0K\u001b[1G87.6 MiB [] 30% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 31% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 32% 1.1s\u001b[0K\u001b[1G87.6 MiB [] 34% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 36% 1.0s\u001b[0K\u001b[1G87.6 MiB [] 37% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 39% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 40% 0.9s\u001b[0K\u001b[1G87.6 MiB [] 42% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 43% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 44% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 46% 0.8s\u001b[0K\u001b[1G87.6 MiB [] 48% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 49% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 51% 0.7s\u001b[0K\u001b[1G87.6 MiB [] 52% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 53% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 54% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 56% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 58% 0.6s\u001b[0K\u001b[1G87.6 MiB [] 59% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 61% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 63% 0.5s\u001b[0K\u001b[1G87.6 MiB [] 65% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 67% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 69% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 71% 0.4s\u001b[0K\u001b[1G87.6 MiB [] 73% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 75% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 77% 0.3s\u001b[0K\u001b[1G87.6 MiB [] 79% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 82% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 84% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 86% 0.2s\u001b[0K\u001b[1G87.6 MiB [] 88% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 89% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 91% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 93% 0.1s\u001b[0K\u001b[1G87.6 MiB [] 95% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 97% 0.0s\u001b[0K\u001b[1G87.6 MiB [] 100% 0.0s\u001b[0K\n",
            "Firefox 132.0 (playwright build v1466) downloaded to /root/.cache/ms-playwright/firefox-1466\n",
            "Downloading Webkit 18.2 (playwright build v2104)\u001b[2m from https://playwright.azureedge.net/builds/webkit/2104/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G95.5 MiB [] 0% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 0% 27.7s\u001b[0K\u001b[1G95.5 MiB [] 0% 18.9s\u001b[0K\u001b[1G95.5 MiB [] 0% 12.7s\u001b[0K\u001b[1G95.5 MiB [] 0% 7.4s\u001b[0K\u001b[1G95.5 MiB [] 1% 4.8s\u001b[0K\u001b[1G95.5 MiB [] 3% 3.3s\u001b[0K\u001b[1G95.5 MiB [] 4% 2.6s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.6s\u001b[0K\u001b[1G95.5 MiB [] 5% 2.5s\u001b[0K\u001b[1G95.5 MiB [] 6% 2.3s\u001b[0K\u001b[1G95.5 MiB [] 8% 2.1s\u001b[0K\u001b[1G95.5 MiB [] 9% 1.9s\u001b[0K\u001b[1G95.5 MiB [] 10% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 11% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 12% 1.8s\u001b[0K\u001b[1G95.5 MiB [] 14% 1.6s\u001b[0K\u001b[1G95.5 MiB [] 15% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 16% 1.5s\u001b[0K\u001b[1G95.5 MiB [] 18% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 19% 1.4s\u001b[0K\u001b[1G95.5 MiB [] 20% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 22% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 23% 1.3s\u001b[0K\u001b[1G95.5 MiB [] 24% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 26% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 27% 1.2s\u001b[0K\u001b[1G95.5 MiB [] 28% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 29% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 31% 1.1s\u001b[0K\u001b[1G95.5 MiB [] 33% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 34% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 36% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 37% 1.0s\u001b[0K\u001b[1G95.5 MiB [] 38% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 39% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 41% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 42% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 43% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 44% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 45% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 47% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 48% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 49% 0.9s\u001b[0K\u001b[1G95.5 MiB [] 51% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 52% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 54% 0.8s\u001b[0K\u001b[1G95.5 MiB [] 55% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 57% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 58% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 59% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 60% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 61% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 63% 0.7s\u001b[0K\u001b[1G95.5 MiB [] 64% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 66% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 67% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 68% 0.6s\u001b[0K\u001b[1G95.5 MiB [] 69% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 70% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 71% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 73% 0.5s\u001b[0K\u001b[1G95.5 MiB [] 74% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 75% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 76% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 77% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 79% 0.4s\u001b[0K\u001b[1G95.5 MiB [] 80% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 81% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 83% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 84% 0.3s\u001b[0K\u001b[1G95.5 MiB [] 85% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 87% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 88% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 90% 0.2s\u001b[0K\u001b[1G95.5 MiB [] 91% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 92% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 93% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 94% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 95% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 96% 0.1s\u001b[0K\u001b[1G95.5 MiB [] 98% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 99% 0.0s\u001b[0K\u001b[1G95.5 MiB [] 100% 0.0s\u001b[0K\n",
            "Webkit 18.2 (playwright build v2104) downloaded to /root/.cache/ms-playwright/webkit-2104\n",
            "Downloading FFMPEG playwright build v1010\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1010/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 0.6s\u001b[0K\u001b[1G2.3 MiB [] 7% 0.4s\u001b[0K\u001b[1G2.3 MiB [] 17% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 31% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 77% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1010 downloaded to /root/.cache/ms-playwright/ffmpeg-1010\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:216:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:753:43)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:851:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/server/registry/index.js:840:43)\n",
            "    at async t.<anonymous> (/usr/local/lib/python3.11/dist-packages/playwright/driver/package/lib/cli/program.js:137:7)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Setup complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEXT SCRAPING USING CRAWL4AI\n"
      ],
      "metadata": {
        "id": "AlueAdwi4u40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import requests\n",
        "import asyncio\n",
        "import json\n",
        "from urllib.parse import urljoin\n",
        "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode\n",
        "from crawl4ai.extraction_strategy import JsonCssExtractionStrategy\n",
        "\n",
        "\n",
        "IMAGE_DIR = \"images\"\n",
        "\n",
        "\n",
        "if not os.path.exists(IMAGE_DIR):\n",
        "    os.makedirs(IMAGE_DIR)\n",
        "\n",
        "def save_image(image_url, index, base_url):\n",
        "    try:\n",
        "        if image_url and not image_url.startswith(\"http\"):\n",
        "            image_url = urljoin(base_url, image_url)\n",
        "        response = requests.get(image_url)\n",
        "        response.raise_for_status()\n",
        "        file_path = os.path.join(IMAGE_DIR, f\"{index}.jpg\")\n",
        "        with open(file_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Image saved to {file_path}\")\n",
        "        return file_path\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Failed to download image: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_json(data, filename=\"books_details.json\"):\n",
        "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "async def main():\n",
        "    schema = {\n",
        "        \"name\": \"Books\",\n",
        "        \"baseSelector\": \"article.product_pod\",\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"name\": \"title\",\n",
        "                \"selector\": \"h3 > a\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"title\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"price\",\n",
        "                \"selector\": \"p.price_color\",\n",
        "                \"type\": \"text\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"availability\",\n",
        "                \"selector\": \"p.instock.availability\",\n",
        "                \"type\": \"text\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"rating\",\n",
        "                \"selector\": \"p\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"class\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"image\",\n",
        "                \"selector\": \"img.thumbnail\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"src\"\n",
        "            },\n",
        "            {\n",
        "                \"name\": \"url\",\n",
        "                \"selector\": \"h3 > a\",\n",
        "                \"type\": \"attribute\",\n",
        "                \"attribute\": \"href\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    base_url = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
        "    current_page = 1\n",
        "    max_pages = 4\n",
        "    all_data = []\n",
        "\n",
        "    async with AsyncWebCrawler() as crawler:\n",
        "        while current_page <= max_pages:\n",
        "            url = base_url.format(current_page)  # Add page number to URL\n",
        "            print(f\"Scraping page {current_page}: {url}\")\n",
        "            result = await crawler.arun(\n",
        "                url=url,\n",
        "                config=CrawlerRunConfig(\n",
        "                    cache_mode=CacheMode.BYPASS,\n",
        "                    extraction_strategy=JsonCssExtractionStrategy(schema),\n",
        "                    excluded_tags=[\"form\", \"header\", \"footer\"],\n",
        "                    wait_for_images=True,\n",
        "                    scan_full_page=True\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if result.success:\n",
        "                data = json.loads(result.extracted_content)\n",
        "                if not data:\n",
        "                    print(\"No data found on this page. Stopping.\")\n",
        "                    break\n",
        "\n",
        "                for index, book in enumerate(data, len(all_data) + 1):\n",
        "\n",
        "                    image_url = book.get(\"image\")\n",
        "                    if image_url:\n",
        "                        image_path = save_image(image_url, index, base_url)\n",
        "                        if image_path:\n",
        "                            book[\"image\"] = image_path\n",
        "                    if \"url\" in book:\n",
        "                        book[\"url\"] = urljoin(base_url, book[\"url\"])\n",
        "\n",
        "                all_data.extend(data)\n",
        "                current_page += 1\n",
        "            else:\n",
        "                print(f\"Failed to crawl the page: {result.error_message}\")\n",
        "                break\n",
        "        save_json(all_data)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0BeIWTJkwND",
        "outputId": "19cf948e-574c-45c0-ce18-1dc1733a2198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INIT].... → Crawl4AI 0.4.3b2\n",
            "Scraping page 1: http://books.toscrape.com/catalogue/page-1.html\n",
            "[FETCH]... ↓ http://books.toscrape.com/catalogue/page-1.html... | Status: True | Time: 3.55s\n",
            "[SCRAPE].. ◆ Processed http://books.toscrape.com/catalogue/page-1.html... | Time: 89ms\n",
            "[EXTRACT]. ■ Completed for http://books.toscrape.com/catalogue/page-1.html... | Time: 0.08097539100026552s\n",
            "[COMPLETE] ● http://books.toscrape.com/catalogue/page-1.html... | Status: True | Total: 4.12s\n",
            "Image saved to images/1.jpg\n",
            "Image saved to images/2.jpg\n",
            "Image saved to images/3.jpg\n",
            "Image saved to images/4.jpg\n",
            "Image saved to images/5.jpg\n",
            "Image saved to images/6.jpg\n",
            "Image saved to images/7.jpg\n",
            "Image saved to images/8.jpg\n",
            "Image saved to images/9.jpg\n",
            "Image saved to images/10.jpg\n",
            "Image saved to images/11.jpg\n",
            "Image saved to images/12.jpg\n",
            "Image saved to images/13.jpg\n",
            "Image saved to images/14.jpg\n",
            "Image saved to images/15.jpg\n",
            "Image saved to images/16.jpg\n",
            "Image saved to images/17.jpg\n",
            "Image saved to images/18.jpg\n",
            "Image saved to images/19.jpg\n",
            "Image saved to images/20.jpg\n",
            "Scraping page 2: http://books.toscrape.com/catalogue/page-2.html\n",
            "[FETCH]... ↓ http://books.toscrape.com/catalogue/page-2.html... | Status: True | Time: 2.91s\n",
            "[SCRAPE].. ◆ Processed http://books.toscrape.com/catalogue/page-2.html... | Time: 43ms\n",
            "[EXTRACT]. ■ Completed for http://books.toscrape.com/catalogue/page-2.html... | Time: 0.036850307999884535s\n",
            "[COMPLETE] ● http://books.toscrape.com/catalogue/page-2.html... | Status: True | Total: 3.46s\n",
            "Image saved to images/21.jpg\n",
            "Image saved to images/22.jpg\n",
            "Image saved to images/23.jpg\n",
            "Image saved to images/24.jpg\n",
            "Image saved to images/25.jpg\n",
            "Image saved to images/26.jpg\n",
            "Image saved to images/27.jpg\n",
            "Image saved to images/28.jpg\n",
            "Image saved to images/29.jpg\n",
            "Image saved to images/30.jpg\n",
            "Image saved to images/31.jpg\n",
            "Image saved to images/32.jpg\n",
            "Image saved to images/33.jpg\n",
            "Image saved to images/34.jpg\n",
            "Image saved to images/35.jpg\n",
            "Image saved to images/36.jpg\n",
            "Image saved to images/37.jpg\n",
            "Image saved to images/38.jpg\n",
            "Image saved to images/39.jpg\n",
            "Image saved to images/40.jpg\n",
            "Scraping page 3: http://books.toscrape.com/catalogue/page-3.html\n",
            "[FETCH]... ↓ http://books.toscrape.com/catalogue/page-3.html... | Status: True | Time: 3.24s\n",
            "[SCRAPE].. ◆ Processed http://books.toscrape.com/catalogue/page-3.html... | Time: 126ms\n",
            "[EXTRACT]. ■ Completed for http://books.toscrape.com/catalogue/page-3.html... | Time: 0.08395458799986955s\n",
            "[COMPLETE] ● http://books.toscrape.com/catalogue/page-3.html... | Status: True | Total: 3.75s\n",
            "Image saved to images/41.jpg\n",
            "Image saved to images/42.jpg\n",
            "Image saved to images/43.jpg\n",
            "Image saved to images/44.jpg\n",
            "Image saved to images/45.jpg\n",
            "Image saved to images/46.jpg\n",
            "Image saved to images/47.jpg\n",
            "Image saved to images/48.jpg\n",
            "Image saved to images/49.jpg\n",
            "Image saved to images/50.jpg\n",
            "Image saved to images/51.jpg\n",
            "Image saved to images/52.jpg\n",
            "Image saved to images/53.jpg\n",
            "Image saved to images/54.jpg\n",
            "Image saved to images/55.jpg\n",
            "Image saved to images/56.jpg\n",
            "Image saved to images/57.jpg\n",
            "Image saved to images/58.jpg\n",
            "Image saved to images/59.jpg\n",
            "Image saved to images/60.jpg\n",
            "Scraping page 4: http://books.toscrape.com/catalogue/page-4.html\n",
            "[FETCH]... ↓ http://books.toscrape.com/catalogue/page-4.html... | Status: True | Time: 2.64s\n",
            "[SCRAPE].. ◆ Processed http://books.toscrape.com/catalogue/page-4.html... | Time: 43ms\n",
            "[EXTRACT]. ■ Completed for http://books.toscrape.com/catalogue/page-4.html... | Time: 0.02996633800012205s\n",
            "[COMPLETE] ● http://books.toscrape.com/catalogue/page-4.html... | Status: True | Total: 3.17s\n",
            "Image saved to images/61.jpg\n",
            "Image saved to images/62.jpg\n",
            "Image saved to images/63.jpg\n",
            "Image saved to images/64.jpg\n",
            "Image saved to images/65.jpg\n",
            "Image saved to images/66.jpg\n",
            "Image saved to images/67.jpg\n",
            "Image saved to images/68.jpg\n",
            "Image saved to images/69.jpg\n",
            "Image saved to images/70.jpg\n",
            "Image saved to images/71.jpg\n",
            "Image saved to images/72.jpg\n",
            "Image saved to images/73.jpg\n",
            "Image saved to images/74.jpg\n",
            "Image saved to images/75.jpg\n",
            "Image saved to images/76.jpg\n",
            "Image saved to images/77.jpg\n",
            "Image saved to images/78.jpg\n",
            "Image saved to images/79.jpg\n",
            "Image saved to images/80.jpg\n",
            "Data saved to books_details.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "# Function to convert JSON data to CSV\n",
        "def json_to_csv(json_data, csv_filename=\"books_details.csv\"):\n",
        "    # Extract headers from the first item in JSON data (assuming all items have the same keys)\n",
        "    headers = json_data[0].keys()\n",
        "\n",
        "    # Open the CSV file for writing\n",
        "    with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
        "\n",
        "        # Write the headers to the CSV file\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write the rows (data) to the CSV file\n",
        "        for row in json_data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Data has been successfully converted to {csv_filename}\")\n",
        "\n",
        "# Example usage: Convert the JSON data to CSV\n",
        "with open(\"books_details.json\", \"r\", encoding='utf-8') as json_file:\n",
        "    data = json.load(json_file)\n",
        "    json_to_csv(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw_a5opllqPy",
        "outputId": "b25fd575-0dcd-462b-859c-1cc1d4a4e292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been successfully converted to books_details.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Path to the directory you want to download\n",
        "dir_path = '/content/images'\n",
        "\n",
        "# Create a zip file of the directory\n",
        "shutil.make_archive('/content/images_archive', 'zip', dir_path)\n",
        "\n",
        "# Download the zip file\n",
        "files.download('/content/images_archive.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OOLSaSpTmDmb",
        "outputId": "69c16573-5ab2-460d-876a-943ce120f2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_077824fd-ae5b-475c-9322-5b40e28a5764\", \"images_archive.zip\", 769274)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYbB5jJsml6X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}